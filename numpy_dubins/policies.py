import numpy as np
import scipy
import scipy.spatial
import time

class ReachabilityLQPolicy:
    def __init__(self, state_dim, action_dim, marginFunc, env, horizon = 50, Rc=1e-5):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.marginFunc =  marginFunc
        self.horizon = horizon
        self.env = env
        self.env.reset()
        
        self.R = np.diag(Rc* np.ones((env.action_dim, )))

        self.tol = 1e-6
        self.eps = 1e-5
        self.max_iters = 30
        self.line_search = "baseline"
    
    def initialize_trajectory(self, obs, nominal_controls):
        margins = np.zeros((self.horizon, ))
        nominal_states = np.zeros((self.horizon + 1, self.state_dim))
        nominal_states[0] = np.array( obs )
        
        for t in range(self.horizon):
            obs, nominal_controls[t] = self.env.step( obs, nominal_controls[t] )
            nominal_states[t+1] = np.array( obs )
        
        return nominal_states, nominal_controls, margins
    
    def forward_pass(self, nominal_states, nominal_controls, K_closed_loop, k_open_loop, alpha=1.0):
        margins = np.zeros((self.horizon, ))
        new_states = np.array(nominal_states)
        new_controls = np.array(nominal_controls)

        obs = np.array( nominal_states[0] )
        for t in range(self.horizon):
            new_controls[t] = new_controls[t] + K_closed_loop[t] @ (obs - nominal_states[t]) + alpha*k_open_loop[t]
            obs, new_controls[t] = self.env.step( obs, new_controls[t] )
            new_states[t+1] = np.array( obs )
        
        reachable_margin, critical_margin, state_margins, critical_index = self.get_margin(new_states, new_controls)
        
        return new_states, new_controls, reachable_margin, critical_margin, state_margins, critical_index
    
    def get_margin(self, nominal_states, nominal_controls):
        R = self.R
        
        reachable_margin = np.inf
        critical_margin = np.inf

        state_margins = np.zeros((self.horizon, ))
        critical_index = -1
        
        t = self.horizon - 1
        # Find margin over truncated horizon
        while t>-1:
            obs_curr = nominal_states[t:t+1]
            action_curr = nominal_controls[t:t+1]
            
            failure_margin, failure_index, failure_subindex = self.marginFunc.eval(obs_curr, action_curr)
            state_margins[t] = failure_margin

            if(failure_margin<reachable_margin):
                critical_index = t
                reachable_margin = failure_margin
                critical_margin = failure_margin

            reachable_margin = reachable_margin - 0.5*nominal_controls[t]@R@nominal_controls[t]
            t = t-1
        return reachable_margin, critical_margin, state_margins, critical_index
        
    def backward_pass(self, nominal_states, nominal_controls, print_index=True):
        # Perform an ILQ backward pass
        V_x = np.zeros((self.horizon+1, self.state_dim))
        V_xx = np.zeros((self.horizon+1, self.state_dim, self.state_dim))
        
        Q_x = np.zeros((self.horizon, self.state_dim))
        Q_u = np.zeros((self.horizon, self.action_dim))
        Q_xx = np.zeros((self.horizon, self.state_dim, self.state_dim))
        Q_uu = np.zeros((self.horizon, self.action_dim, self.action_dim))
        Q_ux = np.zeros((self.horizon, self.action_dim, self.state_dim))
        
        margins = np.zeros((self.horizon, ))
        state_margins = np.zeros((self.horizon, ))
        
        reg_mat = -self.eps*np.eye(self.action_dim)
        
        k_open_loop = np.zeros((self.horizon, self.action_dim))
        K_closed_loop = np.zeros((self.horizon, self.action_dim, self.state_dim))
        
        index_lists = []
        
        R = self.R
        reachable_margin = np.inf
        
        # Backward pass
        t = self.horizon - 1
        while t>-1:
            obs_curr = nominal_states[t:t+1]
            action_curr = nominal_controls[t:t+1]
            
            failure_margin, failure_index, failure_subindex = self.marginFunc.eval(obs_curr, action_curr) 
            
            if(failure_margin<reachable_margin):
                index_lists.append(["Failure", failure_index, failure_subindex])
                active = "Failure"
                reachable_margin = failure_margin
            else:
                index_lists.append(["Propagate", failure_index, failure_subindex])
            reachable_margin = reachable_margin - 0.5*nominal_controls[t]@R@nominal_controls[t]
            margins[t] = reachable_margin
            state_margins[t] = failure_margin
            t = t-1
        
        t = self.horizon - 1
        while t>-1:
            current_index = index_lists[self.horizon - 1 - t]
            obs_curr = nominal_states[t:t+1]
            action_curr = nominal_controls[t:t+1]

            # Failure derivatives            
            c_x_failure = self.marginFunc.dcdx(obs_curr, action_curr, current_index[1], current_index[2])
            c_xx_failure = self.marginFunc.dcdx2(obs_curr, action_curr, current_index[1], current_index[2])
            Ad, Bd, Ac, Bc = self.env.get_jacobian(obs_curr.ravel(), action_curr.ravel())
            
            if(current_index[0]=="Failure"):
                Q_x[t] = c_x_failure
                Q_xx[t] = c_xx_failure
                Q_u[t] = -R@nominal_controls[t] + Bd.T @ V_x[t+1]
                Q_ux[t] = Bd.T @ V_xx[t+1] @ Ad 
                Q_uu[t] = -R@np.eye(action_curr.size) + Bd.T @ V_xx[t+1] @ Bd
                Q_uu_delta = -R@np.eye(action_curr.size) + Bd.T @ ( V_xx[t+1] - self.eps*np.eye(self.state_dim) ) @ Bd
                Q_ux_delta = Bd.T @ ( V_xx[t+1] - self.eps*np.eye(self.state_dim) ) @ Ad
            else:
                Q_x[t] = Ad.T @ V_x[t+1]
                Q_xx[t] = Ad.T @ V_xx[t+1] @ Ad
                Q_u[t] =  -R@nominal_controls[t] + Bd.T @ V_x[t+1]
                Q_ux[t] = Bd.T @ V_xx[t+1] @ Ad  
                Q_uu[t] = -R@np.eye(action_curr.size) + Bd.T @ V_xx[t+1] @ Bd
                Q_uu_delta = -R@np.eye(action_curr.size) + Bd.T @ ( V_xx[t+1] - self.eps*np.eye(self.state_dim) ) @ Bd
                Q_ux_delta = Bd.T @ ( V_xx[t+1] - self.eps*np.eye(self.state_dim) ) @ Ad

            Q_uu_inv = np.linalg.inv( Q_uu_delta )
            
            # Signs for maximization
            k_open_loop[t] =  - Q_uu_inv @ Q_u[t]
            K_closed_loop[t] =  - Q_uu_inv @ Q_ux_delta
            
            # Update value function derivative for the previous time step
            if(current_index[0]=="Failure"):
                V_x_critical = c_x_failure
                V_xx_critical = c_xx_failure
                #V_x[t] = c_x_failure
                #V_xx[t] = c_xx_failure
                V_x[t] = Q_x[t] + Q_ux[t].T @ k_open_loop[t]
                V_xx[t] = Q_xx[t] + Q_ux[t].T @ K_closed_loop[t]
            else:
                V_x[t] = Q_x[t] + Q_ux[t].T @ k_open_loop[t]
                V_xx[t] = Q_xx[t] + Q_ux[t].T @ K_closed_loop[t]

            t = t-1
        
        self.Q_u = Q_u 
        barrier_constraint_data = {"V_xx": V_xx[0], "V_x": V_x[0], "V_xx_critical": V_xx_critical, "V_x_critical": V_x_critical, "V_t": margins[0], "state_margins": state_margins}        
            
        return K_closed_loop, k_open_loop, barrier_constraint_data
    
    def get_action(self, initial_state, initial_controls=None, animate=False):
        start_time = time.time()
        # Get initial trajectory with naive controls
        if initial_controls is None:
            initial_controls = np.zeros((self.horizon, self.action_dim))
            initial_controls[:, 0] = 0.01
        
        states, controls, margins = self.initialize_trajectory(initial_state, initial_controls)

        # Update control with ILQ updates
        iters = 0
        J, critical_margin, state_margins, critical_index = self.get_margin(states, controls)
        
        converged = False
        status = 0
        updated_constraints_data = None

        convergence_sequence = []
        convergence_sequence.append( critical_margin )
        self.margin = 5.0
        while iters<self.max_iters and not converged:
            iters = iters + 1
            # Backward pass
            K_closed_loop, k_open_loop, barrier_constraints_data = self.backward_pass(states, controls, False)
            
            # Choose the best alpha scaling using appropriate line search methods
            alpha_chosen = self.baseline_line_search( states, controls, K_closed_loop, k_open_loop, critical_margin, J )
            
            if alpha_chosen<1e-13:
                J_new = J
                break

            states, controls, J_new, critical_margin, state_margins, critical_index = self.forward_pass(states, controls, K_closed_loop, k_open_loop, alpha_chosen)

            convergence_sequence.append( critical_margin )

            if J_new>0 and np.abs((J_new-J) / J) < self.tol:  # Small improvement.
                converged = True
            J = J_new

        
        # Backward pass
        K_closed, K_open, updated_constraints_data = self.backward_pass(states, controls, False)
        
        process_time = time.time()-start_time
        solver_dict = {"states": states, "controls": controls, "status": status, "margin": critical_margin, "reachable_margin": J, "iteration_no": iters, "critical_constraint_type": 0,
                            "convergence": convergence_sequence, "label": "Optimal safety plan", "id": 'Optimal', "t_process": process_time, "iterations": iters}
        return controls[0], solver_dict, updated_constraints_data

    def baseline_line_search( self, states, controls, K_closed_loop, k_open_loop, critical, J, beta=0.3 ):
        alpha = 1.0
        while alpha>1e-13:
            X, U, J_new, critical_new, _, _ = self.forward_pass(states, controls, K_closed_loop, k_open_loop, alpha)    

            # Accept if there is improvement
            margin_imp = (J_new - J)
            if margin_imp > 0.:
                return alpha
            alpha = beta*alpha

        return alpha

    def armijo_line_search(self, states, controls, K_closed_loop, k_open_loop,  state_margins, critical_index, J, beta=0.3):
        # Armijo only condition - Wolfe condition is not implemented.
        alpha = 1.0

        alpha_converged = False

        margin_old = state_margins[critical_index]

        while not alpha_converged:
            X, U, J_new, critical_margin_new, state_margins_new, critical_index_new  = self.forward_pass(states, controls, K_closed_loop, k_open_loop, alpha)

            # cu is zero so we propagate cx one step back and use Qu as gradient
            grad_u = self.Q_u[critical_index - 1]

            delta_u = K_closed_loop[critical_index-1] @ (X[critical_index-1] - states[critical_index-1]) + k_open_loop[critical_index-1]

            t = 0.5 * grad_u @ delta_u

            if J_new - t*alpha >= J:        
                alpha_converged = True
                return alpha
            else:
                # Reduce alpha and check for decrease condition
                alpha = beta*alpha
                if alpha < 1e-18:
                    # Stop iterating here
                    return alpha

    def trust_region_ratio(self, states, controls, K_closed_loop, k_open_loop, V_x_critical , V_xx_critical, state_margins, critical_index, J, beta=0.6 ):
        alpha = 1.0
        # Margin around which to search for the next descent point.
        alpha_converged = False

        margin_old = state_margins[critical_index]
        while not alpha_converged:
            X, U, J_new, critical_margin_new, state_margins_new, critical_index_new = self.forward_pass(states, controls, K_closed_loop, k_open_loop, alpha)

            # Find new state margin at old critical point
            margin_new = state_margins_new[critical_index]
      
            # Trajectory difference
            traj_diff = max([np.linalg.norm(np.array(x_new) - np.array(x_old)) for x_new, x_old in zip(np.array(X)[:-1,:2], np.array(states)[:-1,:2])])
            
            # Decrease in margin based on second order approximation at pinch point.
            x_diff = np.array([(np.array(x_new) - np.array(x_old)) for x_new, x_old in zip((np.array(X))[critical_index, :], (np.array(states))[critical_index, :] ) ])

            delta_margin_quadratic_approx = 0.5 * (np.transpose(x_diff) @ V_xx_critical[0] + 2 * np.transpose(V_x_critical[0])) @ x_diff

            # Actual decrease and approximated decrease.
            delta_margin_quadratic_actual = margin_new - margin_old
            error = delta_margin_quadratic_approx - delta_margin_quadratic_actual

            if delta_margin_quadratic_approx!=0:
                rho = delta_margin_quadratic_actual / delta_margin_quadratic_approx
            else:
                rho = 0.

            # Trajectory difference is within margin and there is improvement. 
            # Accept new alpha and change margin.
            # These conditions are obtained from Reach-avoid games:
            # https://github.com/SafeRoboticsLab/Reach-Avoid-Games 

            if abs(traj_diff)<self.margin and J_new > J:
                if rho >= 0.85 and abs(traj_diff - self.margin) < 0.01:
                    self.margin = min(1.25 * self.margin, 10)
                elif rho <= 0.15:
                    self.margin = max(0.85 * self.margin, 0.1)
                
                alpha_converged = True
                return alpha
            else:
                alpha = beta * alpha
                if alpha < 1e-19:
                    return alpha
        return alpha